{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit8c540594db5e4821b36bba85d8b74ea6",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle of the Neighborhoods\n",
    "## Coursera Capstone"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction / Business Problem\n",
    "\n",
    "A client wishes to open up a business of his/her choice in the city of Toronto. The client however does not know what optimal location to place their business in. Our task is to select/recommend a suitable location for them."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this assignment, we will look at a location using data such as postal code and clusters from our previous modules. We will combine this data with input from the client (i.e. max radius from nearest business of the same type, type of business) and Foursquare API calls to see similar business in the same viscinity. Combining this data we will use K-Nearest Neighbor from the sci-kit library to show the clusters of businesses and show a representation of this on a Folium map. We will then recommend (2) possible locations for the client:\n",
    "\n",
    "1. A location that is optimized inside the cluster of similar business types since consumers may look to goto that cluster from previous experience to find that particular business.\n",
    "2. A location where there are none of that type of business in that cluster which will introduce no competition for our client since it will only be the client serving their particular kind of customers."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate a map of NYC's 5 boroughs for this Capstone. I originally was going to use Toronto as the basis for this analysis but upon experimenting with Foursquare API, I am unable to return search queries for that area. Potentiall this can be due to COVID-19 having businesses closed for the time being and resulting in no open businesses of the query search. I have had more success using New York City to obtain query searches, therefore I will proceed with NYC. \n",
    "\n",
    "*For Module 3 reference please see*: [Week_3 Notebook](https://github.com/jye0325/Coursera_Capstone/blob/master/Week_3.ipynb)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of NYC Neighborhoods are downloaded"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O index.html https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhoods are then converted to a DataFrame using BeautifulSoup and Pandas"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "fp = open('index.html', encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp, 'html5lib')\n",
    "table = soup.table.prettify()\n",
    "\n",
    "df = pd.read_html(table, na_values=[])\n",
    "df = df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the table to have Zip Codes listing Neighborhoods"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange columns\n",
    "df = df[['ZIP Codes', 'Borough', 'Neighborhood']]\n",
    "df_copy = pd.DataFrame()\n",
    "for index, row in df.iterrows():\n",
    "    splits = row[0].split(',')\n",
    "    for split in splits:\n",
    "        a_row = pd.Series([split.strip(), row[1], row[2]])\n",
    "        a_row = pd.DataFrame([a_row])\n",
    "        df_copy = pd.concat([a_row, df_copy])\n",
    "df_copy.rename(columns={0:'ZIP Code', 1:'Borough', 2:'Neighborhood'}, inplace=True)\n",
    "df_copy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Geocoding API used to obtain latitude and longitude of the Zip Codes.\n",
    "**Note: API Key will not be disclosed in this `.ipynb` file or Github commit!**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Commented out to avoid accidentally making API Calls to Google API for quota purposes.\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "zip_codes = pd.DataFrame()\n",
    "\n",
    "with open('google_cloud_credentials.json', 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "for z in df_copy['ZIP Code']:\n",
    "    url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    data = {\n",
    "        \"address\":\"{}\".format(z),\n",
    "        \"key\":datastore['key']\n",
    "    }\n",
    "    response = requests.get(url, params=data)\n",
    "\n",
    "    data = json.loads(response.text)\n",
    "\n",
    "    location = data['results'][0]['geometry']['location']\n",
    "    \n",
    "    # Create rectangular boundary\n",
    "    northeast = data['results'][0]['geometry']['viewport']['northeast']\n",
    "    southwest = data['results'][0]['geometry']['viewport']['southwest']\n",
    "    top_lat = northeast['lat']\n",
    "    top_lng = northeast['lng']\n",
    "    bot_lat = southwest['lat']\n",
    "    bot_lng = southwest['lng']\n",
    "\n",
    "    c1 = '{}, {}'.format(top_lng, top_lat)\n",
    "    c2 = '{}, {}'.format(bot_lng, top_lat)\n",
    "    c3 = '{}, {}'.format(bot_lng, bot_lat)\n",
    "    c4 = '{}, {}'.format(top_lng, bot_lat)\n",
    "\n",
    "    a_row = pd.Series([z, float(location['lat']), float(location['lng']), c1, c2, c3, c4])\n",
    "    a_row = pd.DataFrame([a_row])\n",
    "    zip_codes = pd.concat([a_row, zip_codes])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_codes.rename(columns={0:'ZIP Code', 1:'Latitude', 2:'Longitude', 3:'c1', 4:'c2', 5:'c3', 6:'c4'}, inplace=True)\n",
    "#zip_codes.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save `zip_codes` DataFrame to Avoid Extra API Calls to Google"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out to avoid accidentally overriding existing .csv file with a blank DataFrame if Kernel is restarted.\n",
    "#zip_codes.to_csv('zip_codes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `zip_codes` into a DataFrame (when restarting kernel or notebook) and modify data types to be compatible"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes = pd.read_csv('zip_codes.csv', index_col=0)\n",
    "zip_codes['ZIP Code'] = zip_codes['ZIP Code'].astype(int)\n",
    "zip_codes['Latitude'] = zip_codes['Latitude'].astype(float)\n",
    "zip_codes['Longitude'] = zip_codes['Longitude'].astype(float)\n",
    "zip_codes['c1'] = zip_codes['c1'].astype(str)\n",
    "zip_codes['c2'] = zip_codes['c2'].astype(str)\n",
    "zip_codes['c3'] = zip_codes['c3'].astype(str)\n",
    "zip_codes['c4'] = zip_codes['c4'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify `df_copy` data types to be compatible"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['ZIP Code'] = df_copy['ZIP Code'].astype(int)\n",
    "df_copy['Borough'] = df_copy['Borough'].astype(str)\n",
    "df_copy['Neighborhood'] = df_copy['Neighborhood'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two DataFrames"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(df_copy, zip_codes, how=\"left\", on=['ZIP Code'])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium Map of our current ZIP Codes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "address = 'New York City, NY'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"ny_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of New York City, NY are {}, {}.'.format(latitude, longitude))\n",
    "\n",
    "# create map of Toronto using latitude and longitude values\n",
    "map_nyc = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, borough, neighborhood in zip(result['Latitude'], result['Longitude'], result['Borough'], result['Neighborhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_nyc)\n",
    "        \n",
    "map_nyc.save(\"map_nyc.html\")\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Map with Zip Codes Populated for Github Viewing\n",
    "\n",
    "![NYC Map with Zip Codes](nyc_map_with_zip_codes.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create cluster based on neighborhoods"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GeoJSON"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "nlist = df['Neighborhood']\n",
    "neighborhood_list = []\n",
    "for neighborhood_name in nlist:\n",
    "    coordinates = []\n",
    "    df_temp = result[result.Neighborhood == neighborhood_name]\n",
    "    for index, row in df_temp.iterrows():\n",
    "        coordinates.append([row['Longitude'],row['Latitude']])\n",
    "    neighborhood_list.append({\"type\": \"Feature\", \"properties\": {\"name\": neighborhood_name}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [coordinates]}})\n",
    "\n",
    "neighborhood = {\"type\": \"FeatureCollection\", \"features\": neighborhood_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_list = []\n",
    "for index, row in result.iterrows():\n",
    "    coordinates = []\n",
    "    row['c1']= row['c1'].split(', ')\n",
    "    row['c2']= row['c2'].split(', ')\n",
    "    row['c3']= row['c3'].split(', ')\n",
    "    row['c4']= row['c4'].split(', ')\n",
    "    n = 0\n",
    "    while n <2:\n",
    "        row['c1'][n]= float(row['c1'][n])\n",
    "        row['c2'][n]= float(row['c2'][n])\n",
    "        row['c3'][n]= float(row['c3'][n])\n",
    "        row['c4'][n]= float(row['c4'][n])\n",
    "        n=n+1\n",
    "    coordinates.append([row['c1'],row['c2'], row['c3'], row['c4']])\n",
    "    zipcode_list.append({\"type\": \"Feature\", \"properties\": {\"name\": row['Neighborhood'], \"zip_code\":row['ZIP Code']}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": coordinates}})\n",
    "geozipcode = {\"type\": \"FeatureCollection\", \"features\": zipcode_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundaries of NYC ZIP Code"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map([40.7127281, -74.0060152], zoom_start=10, tiles='cartodbpositron')\n",
    "folium.GeoJson(geozipcode).add_to(m)\n",
    "folium.LatLngPopup().add_to(m)\n",
    "\n",
    "for i in geozipcode['features']:\n",
    "    polygons = folium.Polygon(locations=i['geometry']['coordinates'], fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.3,).add_to(m)\n",
    "m.save('zip_code_boundaries.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Map with Zip Codes Boundaries for Github Viewing\n",
    "\n",
    "![NYC Map with Zip Code Boundaries](zip_code_boundaries.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us start with importing Foursquare API.\n",
    "For this example we will perform Foursquare API Calls to query `coffee shop` within `50 meter` radius in for the ZIP codes in our DataFrame.\n",
    "We will then populate the data into a DataFrame."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"import json, requests\n",
    "url = 'https://api.foursquare.com/v2/venues/explore'\n",
    "\n",
    "store_db = pd.DataFrame()\n",
    "\n",
    "with open('foursquare_credentials.json', 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "for z, lat, lng in zip(zip_codes['ZIP Code'], zip_codes['Latitude'], zip_codes['Longitude']):\n",
    "    params = dict(\n",
    "    client_id=datastore['client_id'],\n",
    "    client_secret=datastore['client_secret'],\n",
    "    v='20180323',\n",
    "    near='{}'.format(z),\n",
    "    #ll='{},{}'.format(lat, lng),\n",
    "    #ll='40.712,-74.006',\n",
    "    radius=1609.34, # Equivalent to 1 mile\n",
    "    query='coffee shop',\n",
    "    limit=50\n",
    "    )\n",
    "    resp = requests.get(url=url, params=params)\n",
    "    data = json.loads(resp.text)\n",
    "\n",
    "    print(\"For ZIP Code: {}, we found:\".format(z))\n",
    "\n",
    "    for d in data['response']['groups'][0]['items']:\n",
    "        print(d['venue']['name'])\n",
    "        venue_name = d['venue']['name']\n",
    "        venue_id = d['venue']['id']\n",
    "        summary = str(d['reasons']['items'][0]['summary'])\n",
    "        if summary.find('This spot is popular') > 0:\n",
    "            rating = 1 \n",
    "        else: \n",
    "            rating = 0\n",
    "        lat = d['venue']['location']['lat']\n",
    "        lng = d['venue']['location']['lng']\n",
    "        a_row = pd.Series([int(z), str(venue_name), str(venue_id), int(rating), float(lat), float(lng)])\n",
    "        a_row = pd.DataFrame([a_row])\n",
    "        store_db = pd.concat([a_row, store_db])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup the DataFrame"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_db.rename(columns={0:'ZIP Code', 1:'Venue Name', 2:'Venue ID', 3:'Rating', 4:'Latitude', 5:'Longitude'}, inplace=True)\n",
    "#store_db.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save query to `.csv` file"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_db.to_csv('coffee_shops_within_1mi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_db = pd.read_csv('coffee_shops_within_1mi.csv', index_col=0)\n",
    "store_db['Venue Name'] = store_db['Venue Name'].astype(str)\n",
    "store_db['Venue ID'] = store_db['Venue ID'].astype(str)\n",
    "store_db['Rating'] = store_db['Rating'].astype(int)\n",
    "store_db['Latitude'] = store_db['Latitude'].astype(float)\n",
    "store_db['Longitude'] = store_db['Longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any duplicates "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_db.drop_duplicates(subset =\"Venue ID\", inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the populated data in the DataFrame, we will use this to plot the locations on our Folium Map."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of New York City\n",
    "map_nyc = folium.Map(location = [40.7127281, -74.0060152], zoom_start = 10)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "stores = plugins.MarkerCluster().add_to(map_nyc)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label in zip(store_db['Latitude'], store_db['Longitude'], store_db['Venue ID']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=label,\n",
    "    ).add_to(stores)\n",
    "\n",
    "# display map\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_nyc.save('stores.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Map with Stores Populated for Github Viewing\n",
    "\n",
    "![NYC Map with Stores](stores.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1**: We can use Chloropleth Heatmaps to create hotspot neighborhoods for coffee shop counts"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_coffee_shop = pd.merge(store_db, result[['ZIP Code', 'Neighborhood']], how=\"left\", on=['ZIP Code'])\n",
    "count = pd.value_counts(neighborhood_coffee_shop['Neighborhood'])\n",
    "coffee_shop_count = pd.DataFrame({'Neighborhood':count.index, 'Total':count.values})\n",
    "coffee_shop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Neighborhood with the highest amount of coffee shops is {}\".format(coffee_shop_count['Neighborhood'][0]))\n",
    "print(\"The Neighborhood with the fewest amount of coffee shops is {}\".format(coffee_shop_count['Neighborhood'][31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plain world map\n",
    "m = folium.Map(zoom_start=10, location=[40.7127281, -74.0060152])\n",
    "\n",
    "# Generate Choropleth Map\n",
    "folium.Choropleth(\n",
    "    geo_data=geozipcode,\n",
    "    name='choropleth',\n",
    "    data=coffee_shop_count,\n",
    "    columns=['Neighborhood', 'Total'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Coffee Shop Popularity'\n",
    ").add_to(m)\n",
    "\n",
    "# Display Map\n",
    "m.save(\"heat_map.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Map with Heat Map of Stores for Github Viewing\n",
    "\n",
    "![NYC Map with Heat Map of Stores](heat_map.png)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2**: We can create clusters of the coffee shops."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns; sns.set()\n",
    "import csv\n",
    "\n",
    "temp = pd.read_csv('coffee_shops_within_1mi.csv')\n",
    "temp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.dropna(axis=0,how='any',subset=['Latitude','Longitude'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable with the Longitude and Latitude\n",
    "X=temp.loc[:,['Venue ID','Latitude','Longitude']]\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_clusters = range(1,20)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "Y_axis = temp[['Latitude']]\n",
    "X_axis = temp[['Longitude']]\n",
    "score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 25, init ='k-means++')\n",
    "kmeans.fit(X[X.columns[1:3]]) # Compute k-means clustering.\n",
    "X['cluster_label'] = kmeans.fit_predict(X[X.columns[1:3]])\n",
    "centers = kmeans.cluster_centers_ # Coordinates of cluster centers.\n",
    "labels = kmeans.predict(X[X.columns[1:3]]) # Labels of each point\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.plot.scatter(x = 'Latitude', y = 'Longitude', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['Venue ID','cluster_label']]\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data = temp.merge(X, left_on='Venue ID', right_on='Venue ID')\n",
    "clustered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data.to_csv ('clustered_data.csv', index=None, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns; sns.set()\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('properties_2016.csv')\n",
    "df.head(10)\n",
    "\n",
    "df.dropna(axis=0,how='any',subset=['latitude','longitude'],inplace=True)\n",
    "\n",
    "# Variable with the Longitude and Latitude\n",
    "X=df.loc[:,['parcelid','latitude','longitude']]\n",
    "X.head(10)\n",
    "\n",
    "K_clusters = range(1,10)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "Y_axis = df[['latitude']]\n",
    "X_axis = df[['longitude']]\n",
    "score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\n",
    "# Visualize\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()\n",
    "\n",
    "kmeans = KMeans(n_clusters = 3, init ='k-means++')\n",
    "kmeans.fit(X[X.columns[1:3]]) # Compute k-means clustering.\n",
    "X['cluster_label'] = kmeans.fit_predict(X[X.columns[1:3]])\n",
    "centers = kmeans.cluster_centers_ # Coordinates of cluster centers.\n",
    "labels = kmeans.predict(X[X.columns[1:3]]) # Labels of each point\n",
    "X.head(10)\n",
    "\n",
    "X.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "X.head(5)\n",
    "\n",
    "X = X[['parcelid','cluster_label']]\n",
    "X.head(5)\n",
    "\n",
    "clustered_data = df.merge(X, left_on='parcelid', right_on='parcelid')\n",
    "clustered_data.head(5)\n",
    "\n",
    "clustered_data.to_csv ('clustered_data.csv', index=None, header = True)\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pp = store_db[['Latitude', 'Longitude']]\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x=pp['Latitude'], y=pp['Longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "pp = pp[['Latitude','Longitude']].to_numpy()\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = 10, n_init = 12)\n",
    "k_means.fit(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_labels = k_means.labels_\n",
    "k_means_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_cluster_centers = k_means.cluster_centers_\n",
    "k_means_cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the plot with the specified dimensions.\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Colors uses a color map, which will produce an array of colors based on\n",
    "# the number of labels there are. We use set(k_means_labels) to get the\n",
    "# unique labels.\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(set(k_means_labels))))\n",
    "\n",
    "# Create a plot\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# For loop that plots the data points and centroids.\n",
    "# k will range from 0-3, which will match the possible clusters that each\n",
    "# data point is in.\n",
    "for k, col in zip(range(len([[4,4], [-2, -1], [2, -3], [1, 1], [4,5], [5,4], [7,7], [8,8], [9,9], [10,0], [11,10]])), colors):\n",
    "\n",
    "    # Create a list of all data points, where the data poitns that are \n",
    "    # in the cluster (ex. cluster 0) are labeled as true, else they are\n",
    "    # labeled as false.\n",
    "    my_members = (k_means_labels == k)\n",
    "    \n",
    "    # Define the centroid, or cluster center.\n",
    "    cluster_center = k_means_cluster_centers[k]\n",
    "    \n",
    "    # Plots the datapoints with color col.\n",
    "    ax.plot(pp[my_members, 0], pp[my_members, 1], 'w', markerfacecolor=col, marker='.')\n",
    "    \n",
    "    # Plots the centroids with specified color, but with a darker outline\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n",
    "\n",
    "# Title of the plot\n",
    "ax.set_title('KMeans')\n",
    "\n",
    "# Remove x-axis ticks\n",
    "ax.set_xticks(())\n",
    "\n",
    "# Remove y-axis ticks\n",
    "ax.set_yticks(())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Francisco\n",
    "map_nyc = folium.Map(location = [40.7127281, -74.0060152], zoom_start = 10)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "stores = plugins.MarkerCluster().add_to(map_nyc)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label in zip(pp['Latitude'], pp['Longitude'], pp['Cluster Labels']):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=label,\n",
    "    ).add_to(stores)\n",
    "\n",
    "# display map\n",
    "# sanfran_map.save('map_nyc_cluster.html')\n",
    "#map_nyc.save('map_nyc_coffee_shops_5_mi.html')\n",
    "map_nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "pp = store_db[['Latitude', 'Longitude']]\n",
    "# set number of clusters\n",
    "kclusters = 10\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(pp)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:]\n",
    "\n",
    "kmeans.labels_.shape\n",
    "\n",
    "pp.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "# create map\n",
    "map_clusters = folium.Map(location=[40.7127281, -74.0060152], zoom_start=11)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster in zip(pp['Latitude'], pp['Longitude'], result['Neighborhood'], pp['Cluster Labels']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "From the Folium Map with the Heat Map, we can see there is a higher concentration of stores in Manhattan and parts of Queens/Brooklyn. There is a lower concentration of stores in Staten Island, parts of Brooklyn and Queens, and Upper Manhattan and Bronx."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The recommended areas to open up a coffee shop is ______________. The result for this is location based. I feel that this recommendation is very limited in terms of reliability and accuracy. For a more refined recommendation we would need to obtain more data such as population surveys on likelihood of patroning a coffee shop in X,Y,Z neighborhood and so on. Such data is out of the scope of this project and would require additional funding for further studies. Another thing to point out is that for Chloropleth Maps, the ZIP Codes plottted in polygons aren't 100% accurate since they are assembled with rectangles from Google Geocoding API so there are overlapping areas. I was unable to find an actual GeoJSON file with ZIP Codes mapped out."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From this Coursera Capstone project we combined machine learning (K-Means) and data visualization (Folium Maps) to make a recommendation for a location of business. I would like to thank IBM for creating this Cousera Course for introducing me to various Data Science topics using Python and available Python Libraries. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Debugging"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foursquare"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = data['response']['groups'][0]['items'][1]\n",
    "# for loop for ['venue']['name']\n",
    "# may want to filter out duplicates by ['venue']['id']\n",
    "# if ['reasons']['items']['summary'] == 'This spot is popular' score it 1 else score it 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['response']['groups'][0]['items']:\n",
    "    print(i['venue']['location']['lat'])\n",
    "    print(i['reasons']['items'][0]['summary'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Map"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "# let's start again with a clean copy of the map of San Francisco\n",
    "sanfran_map = folium.Map(location = [latitude, longitude], zoom_start = 12)\n",
    "\n",
    "# instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents = plugins.MarkerCluster().add_to(sanfran_map)\n",
    "\n",
    "# loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, label, in zip(result.Latitude, result.Longitude, result.Neighborhood):\n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "        popup=label,\n",
    "    ).add_to(incidents)\n",
    "\n",
    "# display map\n",
    "sanfran_map.save('map_nyc_cluster.html')\n",
    "sanfran_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geozipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}